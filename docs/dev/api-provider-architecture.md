# API Provider Architecture - åœ°ç«¯/API æœå‹™åˆ‡æ›è¨­è¨ˆ

> **æ–‡ä»¶ç‰ˆæœ¬**: 1.0
> **å»ºç«‹æ—¥æœŸ**: 2025-11-27
> **è¨­è¨ˆå“²å­¸**: Linus Torvalds "Good Taste" - æ­£ç¢ºçš„è³‡æ–™çµæ§‹è®“å¯¦ä½œè‡ªç„¶ç°¡å–®
> **ç‹€æ…‹**: âœ… è¨­è¨ˆéšæ®µ â†’ å¾…å¯¦ä½œ

---

## ğŸ¯ è¨­è¨ˆç›®æ¨™

### æ ¸å¿ƒéœ€æ±‚
åœ¨ä¸ç ´å£ç¾æœ‰ WebSocket æµç¨‹çš„å‰æä¸‹ï¼Œæ”¯æ´éˆæ´»åˆ‡æ›ï¼š
- **åœ°ç«¯æ¨¡å¼**: ä½¿ç”¨æœ¬åœ° AI æ¨¡å‹ (Whisper, vLLM, F5-TTS)
- **API æ¨¡å¼**: ä½¿ç”¨é›²ç«¯ API æœå‹™ (OpenAI, Anthropic, ElevenLabs ç­‰)

### è¨­è¨ˆåŸå‰‡ (Linus Philosophy)

#### 1. "Good Taste" - æ¶ˆé™¤ç‰¹æ®Šæƒ…æ³
```python
# âŒ ç³Ÿç³•çš„è¨­è¨ˆ (å……æ»¿ç‰¹æ®Šæƒ…æ³)
if mode == "local":
    result = whisper.transcribe(...)
elif mode == "openai":
    result = openai.transcribe(...)
elif mode == "azure":
    result = azure.transcribe(...)

# âœ… å¥½å“å‘³è¨­è¨ˆ (ç„¡ç‰¹æ®Šæƒ…æ³)
provider = get_stt_provider()  # Factory æ ¹æ“šé…ç½®è¿”å›
result = provider.transcribe(...)  # çµ±ä¸€ä»‹é¢
```

#### 2. "Never Break Userspace" - é›¶ç ´å£æ€§
- WebSocket ç«¯é» (`websocket.py`) ä¸éœ€ä¿®æ”¹ä»»ä½•ä¸€è¡Œ
- æ‰€æœ‰ç¾æœ‰ `get_xxx_service()` å‘¼å«ç¹¼çºŒæœ‰æ•ˆ
- é è¨­è¡Œç‚º (`local` æ¨¡å¼) èˆ‡ç¾åœ¨å®Œå…¨ç›¸åŒ

#### 3. Simplicity - ç°¡æ½”åŸ·å¿µ
- Protocol åªå®šç¾©å¿…è¦æ–¹æ³•
- Factory ç”¨ç’°å¢ƒè®Šæ•¸æ§åˆ¶ï¼Œç„¡éœ€è¤‡é›œé…ç½®
- æ¯å€‹ Provider åªå°ˆæ³¨è‡ªå·±çš„å¯¦ä½œ

---

## ğŸ“ æ¶æ§‹è¨­è¨ˆ

### æ ¸å¿ƒæ¨¡å¼: Protocol + Factory Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WebSocket Handler                         â”‚
â”‚              (websocket.py - ä¸éœ€ä¿®æ”¹)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ å‘¼å« get_xxx_service()
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Service Factory                            â”‚
â”‚         (stt.py, llm.py, tts.py - æœ€å°ä¿®æ”¹)                 â”‚
â”‚                                                               â”‚
â”‚  def get_stt_service() -> STTProvider:                       â”‚
â”‚      if config.STT_PROVIDER == "local":                      â”‚
â”‚          return WhisperSTTProvider()                         â”‚
â”‚      elif config.STT_PROVIDER == "openai":                   â”‚
â”‚          return OpenAISTTProvider()                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ è¿”å›å¯¦ä½œ Protocol çš„ Provider
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Protocol Interface                           â”‚
â”‚                  (protocols.py)                              â”‚
â”‚                                                               â”‚
â”‚  class STTProvider(Protocol):                                â”‚
â”‚      async def transcribe(...) -> tuple[str, dict]           â”‚
â”‚                                                               â”‚
â”‚  class LLMProvider(Protocol):                                â”‚
â”‚      async def chat_stream(...) -> AsyncIterator[str]        â”‚
â”‚                                                               â”‚
â”‚  class TTSProvider(Protocol):                                â”‚
â”‚      async def synthesize(...) -> Path                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ å¯¦ä½œä»‹é¢
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Local Providers         â”‚       API Providers          â”‚
â”‚      (ç¾æœ‰å¯¦ä½œæ”¹å)          â”‚      (æœªä¾†æ“´å±•)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ WhisperSTTProvider         â”‚ â€¢ OpenAISTTProvider          â”‚
â”‚ â€¢ VLLMProvider               â”‚ â€¢ AnthropicLLMProvider       â”‚
â”‚ â€¢ F5TTSProvider              â”‚ â€¢ ElevenLabsTTSProvider      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ å¯¦ä½œè¨ˆç•«

### Phase 1: å®šç¾© Protocol ä»‹é¢

**æ–°æª”æ¡ˆ**: `src/avatar/services/protocols.py`

```python
"""
Service provider protocols (interfaces)

Define contracts for STT/LLM/TTS services.
Enables switching between local models and API services.

Design Philosophy (Linus Torvalds):
- "Good programmers worry about data structures and their relationships"
- Protocol defines the data flow contract
- Implementation details are hidden behind the interface
"""
from typing import Protocol, AsyncIterator, Optional
from pathlib import Path


class STTProvider(Protocol):
    """
    Speech-to-Text provider interface

    Implementations:
    - WhisperSTTProvider: Local faster-whisper (CPU)
    - OpenAISTTProvider: OpenAI Whisper API
    - AzureSTTProvider: Azure Speech Services
    - GoogleSTTProvider: Google Cloud Speech-to-Text
    """

    async def transcribe(
        self,
        audio_path: Path,
        language: Optional[str] = None,
        **kwargs
    ) -> tuple[str, dict]:
        """
        Transcribe audio to text

        Args:
            audio_path: Path to audio file (WAV 16kHz mono recommended)
            language: ISO 639-1 language code (None for auto-detect)
            **kwargs: Provider-specific options

        Returns:
            (transcribed_text, metadata)

            metadata format:
            {
                "language": str,           # Detected or specified language
                "duration": float,         # Audio duration in seconds
                "segments_count": int,     # Number of transcription segments
                "confidence": float,       # Average confidence score (0-1)
                "provider": str,           # Provider name (e.g., "whisper_local")
            }

        Raises:
            FileNotFoundError: Audio file not found
            RuntimeError: Transcription failed
        """
        ...


class LLMProvider(Protocol):
    """
    Large Language Model provider interface

    Implementations:
    - VLLMProvider: Local vLLM with quantized models
    - OpenAILLMProvider: OpenAI GPT models
    - AnthropicLLMProvider: Anthropic Claude models
    - AzureLLMProvider: Azure OpenAI Service
    """

    async def chat_stream(
        self,
        messages: list[dict],
        max_tokens: int = 512,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncIterator[str]:
        """
        Stream chat completion chunks (for lower TTFT)

        Args:
            messages: Chat history in OpenAI format
                      [{"role": "user", "content": "..."}]
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (0.0-2.0)
            **kwargs: Provider-specific options

        Yields:
            Response text chunks (delta only, not cumulative)

        Example:
            async for chunk in llm.chat_stream(messages):
                print(chunk, end="", flush=True)

        Raises:
            RuntimeError: Generation failed
        """
        ...


class TTSProvider(Protocol):
    """
    Text-to-Speech provider interface

    Implementations:
    - F5TTSProvider: Local F5-TTS (fast mode)
    - CosyVoiceTTSProvider: Local CosyVoice2 (HQ mode)
    - ElevenLabsTTSProvider: ElevenLabs API
    - AzureTTSProvider: Azure Text-to-Speech
    - OpenAITTSProvider: OpenAI TTS API
    """

    async def synthesize(
        self,
        text: str,
        output_path: Path,
        ref_audio_path: Optional[Path] = None,
        ref_text: Optional[str] = None,
        **kwargs
    ) -> Path:
        """
        Synthesize speech from text (with optional voice cloning)

        Args:
            text: Text to synthesize
            output_path: Where to save synthesized audio
            ref_audio_path: Reference audio for voice cloning
            ref_text: Reference text matching ref_audio
            **kwargs: Provider-specific options

        Returns:
            Path to synthesized audio file (WAV format)

        Raises:
            RuntimeError: Synthesis failed
        """
        ...

    async def synthesize_fast(
        self,
        text: str,
        voice_profile_name: str,
        output_path: Path,
        **kwargs
    ) -> Path:
        """
        Fast synthesis using pre-registered voice profile

        Args:
            text: Text to synthesize
            voice_profile_name: Name of registered voice profile
            output_path: Where to save synthesized audio
            **kwargs: Provider-specific options

        Returns:
            Path to synthesized audio file (WAV format)

        Raises:
            FileNotFoundError: Voice profile not found
            RuntimeError: Synthesis failed
        """
        ...
```

**è¨­è¨ˆç†ç”±**:
- è¿”å›å€¼çµ±ä¸€ç‚º `(result, metadata)`ï¼Œæ–¹ä¾¿æ—¥èªŒå’Œç›£æ§
- æ”¯æ´ `**kwargs` ä¿ç•™ Provider ç‰¹å®šé¸é …çš„éˆæ´»æ€§
- æ¯å€‹æ–¹æ³•éƒ½æœ‰æ˜ç¢ºçš„éŒ¯èª¤è™•ç†å¥‘ç´„

---

### Phase 2: ç’°å¢ƒè®Šæ•¸é…ç½®

**ä¿®æ”¹**: `src/avatar/core/config.py` (åœ¨ç¬¬ 45 è¡Œä¹‹å¾Œæ–°å¢)

```python
    # ============================================================
    # Service Provider Configuration (åœ°ç«¯/API åˆ‡æ›)
    # ============================================================

    # Provider Mode Selection
    # æ”¯æ´çš„ STT Providers: local (Whisper), openai, azure, google
    STT_PROVIDER: str = os.getenv("AVATAR_STT_PROVIDER", "local")

    # æ”¯æ´çš„ LLM Providers: local (vLLM), openai, anthropic, azure
    LLM_PROVIDER: str = os.getenv("AVATAR_LLM_PROVIDER", "local")

    # æ”¯æ´çš„ TTS Providers: local (F5-TTS), elevenlabs, azure, openai
    TTS_PROVIDER: str = os.getenv("AVATAR_TTS_PROVIDER", "local")

    # -------------------- STT API Configuration --------------------
    STT_API_KEY: Optional[str] = os.getenv("AVATAR_STT_API_KEY")
    STT_API_ENDPOINT: Optional[str] = os.getenv("AVATAR_STT_API_ENDPOINT")
    STT_API_MODEL: str = os.getenv("AVATAR_STT_API_MODEL", "whisper-1")

    # -------------------- LLM API Configuration --------------------
    LLM_API_KEY: Optional[str] = os.getenv("AVATAR_LLM_API_KEY")
    LLM_API_ENDPOINT: Optional[str] = os.getenv("AVATAR_LLM_API_ENDPOINT")
    LLM_API_MODEL: str = os.getenv("AVATAR_LLM_API_MODEL", "gpt-4")
    LLM_API_BASE_URL: Optional[str] = os.getenv("AVATAR_LLM_API_BASE_URL")  # For custom endpoints

    # -------------------- TTS API Configuration --------------------
    TTS_API_KEY: Optional[str] = os.getenv("AVATAR_TTS_API_KEY")
    TTS_API_ENDPOINT: Optional[str] = os.getenv("AVATAR_TTS_API_ENDPOINT")
    TTS_API_VOICE: str = os.getenv("AVATAR_TTS_API_VOICE", "alloy")
    TTS_API_MODEL: str = os.getenv("AVATAR_TTS_API_MODEL", "tts-1")
```

**ä½¿ç”¨ç¯„ä¾‹** (`.env` æª”æ¡ˆ):

```bash
# åœ°ç«¯æ¨¡å¼ (é è¨­)
AVATAR_STT_PROVIDER=local
AVATAR_LLM_PROVIDER=local
AVATAR_TTS_PROVIDER=local

# æ··åˆæ¨¡å¼ (åœ°ç«¯ LLM + API TTS)
AVATAR_STT_PROVIDER=local
AVATAR_LLM_PROVIDER=local
AVATAR_TTS_PROVIDER=elevenlabs
AVATAR_TTS_API_KEY=your_elevenlabs_key

# å…¨ API æ¨¡å¼
AVATAR_STT_PROVIDER=openai
AVATAR_STT_API_KEY=sk-xxx
AVATAR_LLM_PROVIDER=anthropic
AVATAR_LLM_API_KEY=sk-ant-xxx
AVATAR_TTS_PROVIDER=openai
AVATAR_TTS_API_KEY=sk-xxx
```

---

### Phase 3: Factory Pattern é‡æ§‹

#### 3.1 STT Factory

**æ­¥é©Ÿ**:
1. é‡å‘½å: `src/avatar/services/stt.py` â†’ `src/avatar/services/stt_local.py`
2. ä¿®æ”¹é¡åˆ¥å: `STTService` â†’ `WhisperSTTProvider`
3. æ–°å»º: `src/avatar/services/stt.py` (Factory å…¥å£)

**æ–°æª”æ¡ˆ**: `src/avatar/services/stt.py`

```python
"""
STT Service Factory

Returns appropriate STT provider based on AVATAR_STT_PROVIDER configuration.

Linus Principle: "Never break userspace"
- Existing get_stt_service() calls continue to work
- Default behavior (local mode) is unchanged
- Adding new providers requires zero changes to callers
"""
import structlog
from avatar.core.config import config
from avatar.services.protocols import STTProvider

logger = structlog.get_logger()

# Singleton instance
_stt_service: STTProvider | None = None


async def get_stt_service() -> STTProvider:
    """
    Get STT service instance (singleton)

    Returns appropriate provider based on AVATAR_STT_PROVIDER env var.

    Supported providers:
    - local: Whisper (faster-whisper) on CPU
    - openai: OpenAI Whisper API
    - azure: Azure Speech Services
    - google: Google Cloud Speech-to-Text

    Returns:
        STTProvider implementation

    Raises:
        ValueError: Unknown provider
        RuntimeError: Provider initialization failed

    Example:
        stt = await get_stt_service()
        text, metadata = await stt.transcribe(audio_path)
    """
    global _stt_service

    if _stt_service is not None:
        return _stt_service

    provider = config.STT_PROVIDER.lower()

    if provider == "local":
        from avatar.services.stt_local import WhisperSTTProvider
        logger.info("stt.factory.init",
                   provider="local",
                   model=config.WHISPER_MODEL_SIZE,
                   device=config.WHISPER_DEVICE)
        _stt_service = await WhisperSTTProvider.create(
            model_size=config.WHISPER_MODEL_SIZE,
            device=config.WHISPER_DEVICE,
            compute_type=config.WHISPER_COMPUTE_TYPE
        )

    # æœªä¾†æ“´å±•é» (é¸å‹å®Œæˆå¾Œè§£é™¤è¨»é‡‹)
    # elif provider == "openai":
    #     from avatar.services.stt_openai import OpenAISTTProvider
    #     logger.info("stt.factory.init", provider="openai", model=config.STT_API_MODEL)
    #     _stt_service = OpenAISTTProvider(
    #         api_key=config.STT_API_KEY,
    #         model=config.STT_API_MODEL,
    #         endpoint=config.STT_API_ENDPOINT
    #     )

    # elif provider == "azure":
    #     from avatar.services.stt_azure import AzureSTTProvider
    #     logger.info("stt.factory.init", provider="azure")
    #     _stt_service = AzureSTTProvider(
    #         api_key=config.STT_API_KEY,
    #         endpoint=config.STT_API_ENDPOINT
    #     )

    # elif provider == "google":
    #     from avatar.services.stt_google import GoogleSTTProvider
    #     logger.info("stt.factory.init", provider="google")
    #     _stt_service = GoogleSTTProvider(
    #         api_key=config.STT_API_KEY
    #     )

    else:
        raise ValueError(
            f"Unknown STT provider: '{provider}'\n"
            f"Supported providers: local, openai, azure, google\n"
            f"Set AVATAR_STT_PROVIDER environment variable"
        )

    logger.info("stt.factory.ready", provider=provider)
    return _stt_service
```

**ä¿®æ”¹**: `src/avatar/services/stt_local.py` (åŸ `stt.py`)

åªéœ€ä¿®æ”¹é¡åˆ¥å:
```python
# ç¬¬ 20 è¡Œ
class WhisperSTTProvider:  # åŸ: STTService
    """
    Local Whisper STT Provider

    Uses faster-whisper for CPU-based transcription.
    Implements STTProvider protocol.
    """
    # ... ç¾æœ‰å¯¦ä½œå®Œå…¨ä¸è®Š ...
```

#### 3.2 LLM Factory

**æ­¥é©Ÿ**:
1. é‡å‘½å: `llm.py` â†’ `llm_local.py`
2. ä¿®æ”¹é¡åˆ¥å: `LLMService` â†’ `VLLMProvider`
3. æ–°å»º: `llm.py` (Factory)

**æ–°æª”æ¡ˆ**: `src/avatar/services/llm.py`

```python
"""
LLM Service Factory

Returns appropriate LLM provider based on AVATAR_LLM_PROVIDER configuration.
"""
import structlog
from avatar.core.config import config
from avatar.services.protocols import LLMProvider

logger = structlog.get_logger()

_llm_service: LLMProvider | None = None


async def get_llm_service() -> LLMProvider:
    """
    Get LLM service instance (singleton)

    Supported providers:
    - local: vLLM with quantized models (AWQ)
    - openai: OpenAI GPT models
    - anthropic: Anthropic Claude models
    - azure: Azure OpenAI Service
    """
    global _llm_service

    if _llm_service is not None:
        return _llm_service

    provider = config.LLM_PROVIDER.lower()

    if provider == "local":
        from avatar.services.llm_local import VLLMProvider
        logger.info("llm.factory.init",
                   provider="local",
                   model=config.VLLM_MODEL,
                   gpu_memory=config.VLLM_GPU_MEMORY)
        _llm_service = await VLLMProvider.create(
            model_name=config.VLLM_MODEL,
            gpu_memory_utilization=config.VLLM_GPU_MEMORY,
            max_model_len=config.VLLM_MAX_TOKENS
        )

    # æœªä¾†æ“´å±•é»
    # elif provider == "openai": ...
    # elif provider == "anthropic": ...
    # elif provider == "azure": ...

    else:
        raise ValueError(
            f"Unknown LLM provider: '{provider}'\n"
            f"Supported providers: local, openai, anthropic, azure"
        )

    logger.info("llm.factory.ready", provider=provider)
    return _llm_service
```

#### 3.3 TTS Factory

**æ­¥é©Ÿ**:
1. é‡å‘½å: `tts.py` â†’ `tts_local.py`
2. ä¿®æ”¹é¡åˆ¥å: `TTSService` â†’ `F5TTSProvider`
3. æ–°å»º: `tts.py` (Factory)

**æ–°æª”æ¡ˆ**: `src/avatar/services/tts.py`

```python
"""
TTS Service Factory

Returns appropriate TTS provider based on AVATAR_TTS_PROVIDER configuration.
"""
import structlog
from avatar.core.config import config
from avatar.services.protocols import TTSProvider

logger = structlog.get_logger()

_tts_service: TTSProvider | None = None


async def get_tts_service() -> TTSProvider:
    """
    Get TTS service instance (singleton)

    Supported providers:
    - local: F5-TTS (fast mode)
    - elevenlabs: ElevenLabs API
    - openai: OpenAI TTS API
    - azure: Azure Text-to-Speech
    """
    global _tts_service

    if _tts_service is not None:
        return _tts_service

    provider = config.TTS_PROVIDER.lower()

    if provider == "local":
        from avatar.services.tts_local import F5TTSProvider
        logger.info("tts.factory.init", provider="local")
        _tts_service = await F5TTSProvider.create()

    # æœªä¾†æ“´å±•é»
    # elif provider == "elevenlabs": ...
    # elif provider == "openai": ...
    # elif provider == "azure": ...

    else:
        raise ValueError(
            f"Unknown TTS provider: '{provider}'\n"
            f"Supported providers: local, elevenlabs, openai, azure"
        )

    logger.info("tts.factory.ready", provider=provider)
    return _tts_service
```

---

## ğŸ“Š æª”æ¡ˆçµæ§‹è®Šæ›´

### ä¿®æ”¹å‰
```
src/avatar/services/
â”œâ”€â”€ stt.py              # STTService é¡åˆ¥
â”œâ”€â”€ llm.py              # LLMService é¡åˆ¥
â”œâ”€â”€ tts.py              # TTSService é¡åˆ¥
â””â”€â”€ tts_hq.py           # CosyVoice HQ
```

### ä¿®æ”¹å¾Œ
```
src/avatar/services/
â”œâ”€â”€ protocols.py        # âœ¨ æ–°å¢ - Protocol ä»‹é¢å®šç¾©
â”‚
â”œâ”€â”€ stt.py              # ğŸ”„ é‡æ§‹ - Factory å…¥å£
â”œâ”€â”€ stt_local.py        # ğŸ“ é‡å‘½å - WhisperSTTProvider (åŸ stt.py)
â”‚
â”œâ”€â”€ llm.py              # ğŸ”„ é‡æ§‹ - Factory å…¥å£
â”œâ”€â”€ llm_local.py        # ğŸ“ é‡å‘½å - VLLMProvider (åŸ llm.py)
â”‚
â”œâ”€â”€ tts.py              # ğŸ”„ é‡æ§‹ - Factory å…¥å£
â”œâ”€â”€ tts_local.py        # ğŸ“ é‡å‘½å - F5TTSProvider (åŸ tts.py)
â””â”€â”€ tts_hq.py           # âœ… ä¸è®Š - CosyVoice HQ

# æœªä¾†æ“´å±• (é¸å‹å¾Œæ–°å¢)
â”œâ”€â”€ stt_openai.py       # OpenAI Whisper API
â”œâ”€â”€ stt_azure.py        # Azure Speech Services
â”œâ”€â”€ llm_openai.py       # OpenAI GPT
â”œâ”€â”€ llm_anthropic.py    # Anthropic Claude
â”œâ”€â”€ tts_elevenlabs.py   # ElevenLabs API
â””â”€â”€ tts_openai.py       # OpenAI TTS API
```

---

## âœ… å‘å¾Œå…¼å®¹æ€§ä¿è­‰

### WebSocket é›¶æ”¹å‹•
`src/avatar/api/websocket.py` çš„æ‰€æœ‰å‘¼å«ä¿æŒä¸è®Š:

```python
# Line 304: STT å‘¼å«
stt = await get_stt_service()
text, metadata = await stt.transcribe(audio_path, ...)

# Line 342: LLM å‘¼å«
llm = await get_llm_service()
async for chunk in llm.chat_stream(messages, ...):
    ...

# Line 399: TTS å‘¼å«
tts = await get_tts_service()
await tts.synthesize(text, output_path, ...)
```

### å‡½å¼ç°½åä¸è®Š
- `get_stt_service()` â†’ è¿”å› `STTProvider`
- `get_llm_service()` â†’ è¿”å› `LLMProvider`
- `get_tts_service()` â†’ è¿”å› `TTSProvider`

### é è¨­è¡Œç‚ºä¸è®Š
æ‰€æœ‰ç’°å¢ƒè®Šæ•¸é è¨­ç‚º `local`ï¼Œè¡Œç‚ºèˆ‡ç¾åœ¨å®Œå…¨ç›¸åŒã€‚

---

## ğŸ§ª æ¸¬è©¦ç­–ç•¥

### å–®å…ƒæ¸¬è©¦
æ¯å€‹ Provider ç¨ç«‹æ¸¬è©¦:
```python
# tests/unit/services/test_stt_local.py
async def test_whisper_transcribe():
    provider = WhisperSTTProvider(...)
    text, metadata = await provider.transcribe(sample_audio)
    assert text == "expected transcription"
    assert metadata["language"] == "en"
```

### æ•´åˆæ¸¬è©¦
æ¸¬è©¦ Factory åˆ‡æ›:
```python
# tests/integration/test_service_factory.py
async def test_stt_factory_local():
    os.environ["AVATAR_STT_PROVIDER"] = "local"
    stt = await get_stt_service()
    assert isinstance(stt, WhisperSTTProvider)

async def test_stt_factory_openai():
    os.environ["AVATAR_STT_PROVIDER"] = "openai"
    os.environ["AVATAR_STT_API_KEY"] = "sk-test"
    stt = await get_stt_service()
    assert isinstance(stt, OpenAISTTProvider)
```

### E2E æ¸¬è©¦
WebSocket æ•´åˆæ¸¬è©¦ä¸éœ€ä¿®æ”¹ï¼Œè‡ªå‹•æ¶µè“‹æ–°æ¶æ§‹ã€‚

---

## ğŸš€ æœªä¾†æ“´å±•æŒ‡å—

### æ–°å¢ API Provider æ¨™æº–æµç¨‹

ä»¥æ–°å¢ OpenAI STT ç‚ºä¾‹:

#### Step 1: å»ºç«‹ Provider æª”æ¡ˆ
```bash
touch src/avatar/services/stt_openai.py
```

#### Step 2: å¯¦ä½œ Protocol
```python
# src/avatar/services/stt_openai.py
from avatar.services.protocols import STTProvider
import httpx

class OpenAISTTProvider:
    """OpenAI Whisper API Provider"""

    def __init__(self, api_key: str, model: str = "whisper-1"):
        self.api_key = api_key
        self.model = model
        self.client = httpx.AsyncClient()

    async def transcribe(
        self,
        audio_path: Path,
        language: Optional[str] = None,
        **kwargs
    ) -> tuple[str, dict]:
        with open(audio_path, "rb") as f:
            response = await self.client.post(
                "https://api.openai.com/v1/audio/transcriptions",
                headers={"Authorization": f"Bearer {self.api_key}"},
                files={"file": f},
                data={"model": self.model, "language": language or ""}
            )

        result = response.json()

        return (
            result["text"],
            {
                "language": result.get("language", "unknown"),
                "duration": result.get("duration", 0),
                "segments_count": 1,
                "confidence": 1.0,
                "provider": "openai_whisper"
            }
        )
```

#### Step 3: Factory è§£é™¤è¨»é‡‹ (3 è¡Œ)
```python
# src/avatar/services/stt.py
elif provider == "openai":
    from avatar.services.stt_openai import OpenAISTTProvider
    _stt_service = OpenAISTTProvider(
        api_key=config.STT_API_KEY,
        model=config.STT_API_MODEL
    )
```

#### Step 4: è¨­å®šç’°å¢ƒè®Šæ•¸
```bash
export AVATAR_STT_PROVIDER=openai
export AVATAR_STT_API_KEY=sk-xxx
```

#### Step 5: é‡å•Ÿæœå‹™
```bash
poetry run python -m avatar.main
```

**å®Œæˆï¼** ç„¡éœ€ä¿®æ”¹ä»»ä½•å…¶ä»–ç¨‹å¼ç¢¼ã€‚

---

## ğŸ“ˆ æ•ˆç›Šè©•ä¼°

### é–‹ç™¼æ•ˆç‡
- **æ–°å¢ Provider**: 1 å€‹æª”æ¡ˆ + 3 è¡Œ Factory ç¨‹å¼ç¢¼
- **åˆ‡æ› Provider**: 1 å€‹ç’°å¢ƒè®Šæ•¸
- **æ¸¬è©¦éš”é›¢**: æ¯å€‹ Provider ç¨ç«‹æ¸¬è©¦

### ç¨‹å¼ç¢¼å“è³ª
- **è€¦åˆåº¦**: æ¥µä½ (Protocol éš”é›¢)
- **å¯ç¶­è­·æ€§**: é«˜ (å–®ä¸€è·è²¬)
- **å¯æ“´å±•æ€§**: æ¥µé«˜ (é–‹æ”¾å°é–‰åŸå‰‡)

### æŠ€è¡“å‚µå‹™
- **æ–°å¢å‚µå‹™**: ç„¡
- **æ¶ˆé™¤å‚µå‹™**: ç¾æœ‰ç¡¬ç·¨ç¢¼çš„æ¨¡å‹ä¾è³´

---

## ğŸ¯ æª¢æŸ¥æ¸…å–®

åœ¨é–‹å§‹å¯¦ä½œå‰ï¼Œç¢ºèª:

- [x] è¨­è¨ˆç¬¦åˆ Linus "Good Taste" åŸå‰‡
- [x] å‘å¾Œå…¼å®¹æ€§ 100%
- [x] WebSocket ç„¡éœ€ä¿®æ”¹
- [x] é è¨­è¡Œç‚ºä¸è®Š
- [x] æ“´å±•è·¯å¾‘æ¸…æ™°
- [x] éŒ¯èª¤è™•ç†æ˜ç¢º
- [x] æ¸¬è©¦ç­–ç•¥å®Œæ•´

---

## ğŸ“š åƒè€ƒè³‡æ–™

### Linus Torvalds è¨­è¨ˆå“²å­¸
- ["Good Taste" in Coding](https://www.youtube.com/watch?v=o8NPllzkFhE) - TED Talk
- ["Never Break Userspace"](https://lkml.org/lkml/2012/12/23/75) - LKML
- Linux Kernel Coding Style

### Design Patterns
- Protocol-Oriented Programming (PEP 544)
- Factory Pattern (GoF)
- Strategy Pattern (Behavioral)

---

**æ–‡ä»¶ç‹€æ…‹**: âœ… è¨­è¨ˆå®Œæˆï¼Œç­‰å¾…å¯¦ä½œæ‰¹å‡†
**é è¨ˆå¯¦ä½œæ™‚é–“**: 30 åˆ†é˜
**é¢¨éšªè©•ä¼°**: ğŸŸ¢ ä½é¢¨éšª (ç¬¦åˆé›¶ç ´å£åŸå‰‡)
